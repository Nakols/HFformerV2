{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKMHFvHk8ubA",
        "outputId": "3ca269e3-7a67-46b5-f5d8-ba3de4abdebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
            "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip --quiet install pytorch-warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "orio-YPdlbsf"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output \n",
        "!pip --quiet install pytorch_spiking pytorch_lightning \n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "z68p_q4eISQP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import sys\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules import Transformer, TransformerEncoder, TransformerEncoderLayer, LayerNorm\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "import pytorch_spiking\n",
        "import pytorch_warmup as warmup\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FUcYIRwMIVPV"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "  \"plots\": {\n",
        "        \"show_plots\": False,\n",
        "        \"xticks_interval\": 1200,\n",
        "        \"color_actual\": \"#001f3f\",\n",
        "        \"color_train\": \"#3D9970\",\n",
        "        \"color_val\": \"#0074D9\",\n",
        "        \"color_test\": \"#FF4136\",\n",
        "        \"color_pred_train\": \"#3D9970\",\n",
        "        \"color_pred_val\": \"#0074D9\",\n",
        "        \"color_pred_test\": \"#FF4136\",\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"train_split_size\": 0.80,\n",
        "        \"input_window\": 150,\n",
        "        \"output_window\": 50,\n",
        "        \"train_batch_size\": 32,\n",
        "        \"eval_batch_size\": 1,\n",
        "        \"scaler\": \"normal\"\n",
        "    }, \n",
        "    \"model_transformer\": {\n",
        "        \"feature_size\": 250,\n",
        "        \"nhead\": 10,\n",
        "        \"num_layers\": 2,\n",
        "        \"dropout\": 0.2,\n",
        "        \"out_features\": 1,\n",
        "        \"init_range\": 2, #0.5\n",
        "        \"lr\": 0.0002, #0.0001,\n",
        "        \"loss\": \"dilate\"\n",
        "    },\n",
        "    \"paths\": {\n",
        "        \"drive\": {\n",
        "            \"agg_trade\": {\n",
        "                \"train\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/input_data/\",\n",
        "                \"test\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/input_data/\", \n",
        "            },\n",
        "            \"orderbook\": {\n",
        "                \"train\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/input_data/\",\n",
        "                \"test\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/input_data/\",\n",
        "            },\n",
        "            \"models\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/models/\",\n",
        "            \"figures\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/figures/\",\n",
        "            \"utils\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/utils/\",\n",
        "        },\n",
        "        \"local\": {\n",
        "            \"agg_trade\": {\n",
        "                \"train\": \"./input_data/\",\n",
        "                \"test\": \"./input_data/\", \n",
        "            },\n",
        "            \"orderbook\": {\n",
        "                \"train\": \"./input_data/\",\n",
        "                \"test\": \"./input_data/\",\n",
        "            },\n",
        "            \"models\": \"./models/\",\n",
        "            \"figures\": \"./figures/\",\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uNNtAHKTjmDD"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "drive = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rxx4SuGIjmDE",
        "outputId": "40846a92-8f30-4adb-ae38-d17f4f36bb3f"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/wh/v19kyh554sxg44m620vsh05c0000gn/T/ipykernel_18169/1408506528.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H3a86TCSFb8"
      },
      "source": [
        "## Data preparation: augmenting raw financial data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AGOE00q-ARLc"
      },
      "outputs": [],
      "source": [
        "def augment_trade_data(df, lag, forecast_window=None):\n",
        "    '''\n",
        "    Augmenting input data.\n",
        "    '''\n",
        "    if forecast_window:\n",
        "        df['lag_return'] = np.log(df['price'].shift(forecast_window)/df['price'].shift(forecast_window+1))\n",
        "        return df.iloc[forecast_window+1:,:]\n",
        "    if lag == 0:\n",
        "        return df\n",
        "    else:\n",
        "        col_name = 'log_lag'+str(lag)+'_price'\n",
        "        df[col_name] = np.log(df.price) - np.log(df.price).shift(lag)\n",
        "        return df.iloc[lag:,:]\n",
        "\n",
        "def select_features(df_train, df_test, features):\n",
        "    '''\n",
        "    Selecting relevant features.\n",
        "    '''\n",
        "    train_data = df_train[features]\n",
        "    test_data = df_test[features]\n",
        "    return train_data, test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4IH7QdtjmDH"
      },
      "source": [
        "## Defining Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "62t7Yti8sheM"
      },
      "outputs": [],
      "source": [
        "# The following helper functions and models were based on this repo: https://github.com/AIStream-Peelout/flow-forecast\n",
        "\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, n_time_series, seq_len=100, output_seq_len=50, d_model=128,\n",
        "                 n_heads=10, n_encoder_layers=2, n_decoder_layers=2, dropout=0.1, dim_feedforward=2048):\n",
        "        super(SimpleTransformer, self).__init__()\n",
        "        self.n_time_series = n_time_series\n",
        "        self.seq_len = seq_len\n",
        "        self.output_seq_len = output_seq_len\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.n_encoder_layers = n_encoder_layers\n",
        "        self.n_decoder_layers = n_decoder_layers\n",
        "        self.dropout = dropout\n",
        "        self.dim_feedforward = dim_feedforward\n",
        "\n",
        "        self.mask = generate_square_subsequent_mask(self.seq_len).to(device)\n",
        "        self.dense_shape = nn.Linear(self.n_time_series, self.d_model)\n",
        "        self.pe = SimplePositionalEncoding(self.d_model)\n",
        "        self.transformer = Transformer(self.d_model, nhead=self.n_heads, dropout=self.dropout, \n",
        "                                       num_encoder_layers=self.n_encoder_layers,\n",
        "                                       num_decoder_layers=self.n_decoder_layers, activation=pytorch_spiking.SpikingActivation(nn.PReLU()),\n",
        "                                        dim_feedforward=self.dim_feedforward)\n",
        "        self.final_layer = nn.Linear(self.d_model, 1)\n",
        "        self.tgt_mask = generate_square_subsequent_mask(self.output_seq_len).to(device)\n",
        "\n",
        "    def forward(self, x, t, tgt_mask=None, src_mask=None):\n",
        "        x = self.encode_sequence(x[:, :-1, :], src_mask)\n",
        "        return self.decode_seq(x, t, tgt_mask)\n",
        "\n",
        "    def pre_encode(self, x):\n",
        "        x = self.dense_shape(x)\n",
        "        x = self.pe(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        return x\n",
        "\n",
        "    def encode_sequence(self, x, src_mask=None):\n",
        "        x = self.pre_encode(x)\n",
        "        x = self.transformer.encoder(x, src_mask)\n",
        "        return x\n",
        "\n",
        "    def decode_seq(self, mem, t, tgt_mask=None, view_number=None):\n",
        "        if view_number is None:\n",
        "            view_number = self.output_seq_len\n",
        "        if tgt_mask is None:\n",
        "            tgt_mask = self.tgt_mask\n",
        "        t = self.pre_encode(t)\n",
        "        x = torch.Tensor(0)\n",
        "        x = self.transformer.decoder(t, mem, tgt_mask=tgt_mask)\n",
        "        x = self.final_layer(x)\n",
        "        x = x.view(-1, view_number)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "class SimplePositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
        "        super(SimplePositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "        #return self.dropout(x)\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    \"\"\" Generates a square mask for the sequence. The masked positions are filled with float('-inf').\n",
        "        Unmasked positions are filled with float(0.0).\n",
        "    \"\"\"\n",
        "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def greedy_decode(model, src, max_len, real_target, device, unsqueeze_dim=1):\n",
        "    \"\"\"\n",
        "    Mechanism to sequentially decode the model\n",
        "    \"\"\"\n",
        "    src = src.float()\n",
        "    real_target = real_target.float()\n",
        "    if hasattr(model, \"mask\"):\n",
        "        src_mask = model.mask\n",
        "    memory = model.encode_sequence(src, src_mask)\n",
        "    # Get last element of src array to forecast from\n",
        "    ys = src[:, -1, :].unsqueeze(unsqueeze_dim)\n",
        "    for i in range(max_len):\n",
        "        mask = generate_square_subsequent_mask(i + 1).to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model.decode_seq(memory, Variable(ys), Variable(mask), i + 1)\n",
        "            real_target[:, i, 0] = out[:, i]\n",
        "            src = torch.cat((src, real_target[:, i, :].unsqueeze(1)), 1)\n",
        "            ys = torch.cat((ys, real_target[:, i, :].unsqueeze(1)), 1)\n",
        "        memory = model.encode_sequence(src[:, i + 1:, :], src_mask)\n",
        "    return ys[:, 1:, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN1amH-62AC7"
      },
      "source": [
        "## Defining CSV Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dm4-l-wp4x9N"
      },
      "outputs": [],
      "source": [
        "class CSVDataLoader(Dataset):\n",
        "    def __init__(self, df, forecast_history, forecast_length, target_col, start_stamp=0, end_stamp=None, LAG=0):\n",
        "        \"\"\"\n",
        "        A data loader that takes a CSV file and properly batches for use in training/eval a PyTorch model\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.forecast_history = forecast_history\n",
        "        self.forecast_length = forecast_length\n",
        "        self.LAG = LAG\n",
        "        self.df = df.copy()\n",
        "        if start_stamp != 0 and end_stamp is not None:\n",
        "            if self.LAG == 0:\n",
        "                self.df = self.df[start_stamp:end_stamp]\n",
        "            else:\n",
        "                self.df = self.df[start_stamp:end_stamp+self.LAG]\n",
        "        elif start_stamp != 0:\n",
        "            self.df = self.df[start_stamp:]\n",
        "        elif end_stamp is not None:\n",
        "            if self.LAG == 0:\n",
        "                self.df = self.df[:end_stamp]\n",
        "            else:\n",
        "                self.df = self.df[:end_stamp+self.LAG]\n",
        "        if (len(self.df) - self.df.count()).max() != 0:\n",
        "            print('Missing values in data.')\n",
        "            print(len(self.df) - self.df.count())\n",
        "        self.counter = 0\n",
        "        self.targ_col = target_col\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rows = self.df.iloc[idx: self.forecast_history + idx].copy().to_numpy()\n",
        "        epsilon = 0.0001\n",
        "        targs_idx_start = self.forecast_history + idx\n",
        "        if self.LAG == 0:\n",
        "            targ_rows = self.df.iloc[targs_idx_start: self.forecast_length + targs_idx_start].copy().to_numpy()\n",
        "        else:\n",
        "            future_prices = self.df.iloc[targs_idx_start + self.LAG - 1: targs_idx_start + self.LAG].copy().to_numpy()\n",
        "            targ_rows = future_prices.copy()\n",
        "            future_return = np.log(future_prices[0,0]/rows[-1:,0])*10_000\n",
        "            targ_rows[0,0] = future_return\n",
        "        src_data = rows\n",
        "        src_std = np.std(src_data, axis = 0)+epsilon\n",
        "        src_median = np.mean(src_data, axis = 0)\n",
        "        src_std, src_median = src_std.flatten(), src_median.flatten()\n",
        "        trg_dat = targ_rows\n",
        "        src_data_medianized = torch.from_numpy((src_data-src_median)/src_std).float()\n",
        "        src_median[0], src_std[0] = 0, 1.0\n",
        "        trg_data_medianized = torch.from_numpy((trg_dat-src_median)/src_std).float()\n",
        "        return src_data_medianized, trg_data_medianized, src_data\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.LAG == 0:\n",
        "            return (len(self.df) - self.forecast_history - self.forecast_length - 1)\n",
        "        else:\n",
        "            return (len(self.df) - self.forecast_history - self.LAG - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fydGNgID2Fsj"
      },
      "source": [
        "## Defining Custom Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iH5GB6Lz507o"
      },
      "outputs": [],
      "source": [
        "class MASELoss(nn.Module):\n",
        "    def __init__(self, baseline_method):\n",
        "        \"\"\"\n",
        "        This implements the MASE loss function (e.g. MAE_MODEL/MAE_NAIEVE)\n",
        "        \"\"\"\n",
        "        super(MASELoss, self).__init__()\n",
        "        self.method_dict = {\"mean\": lambda x, y: torch.mean(x, 1).unsqueeze(1).repeat(1, y[1], 1)}\n",
        "        self.baseline_method = self.method_dict[baseline_method]\n",
        "\n",
        "    def forward(self, target, output, train_data, m=1):\n",
        "        if len(train_data.shape) < 3:\n",
        "            train_data = train_data.unsqueeze(0)\n",
        "        if m == 1 and len(target.shape) == 1:\n",
        "            output = output.unsqueeze(0)\n",
        "            output = output.unsqueeze(2)\n",
        "            target = target.unsqueeze(0)\n",
        "            target = target.unsqueeze(2)\n",
        "        if len(target.shape) == 2:\n",
        "            output = output.unsqueeze(0)\n",
        "            target = target.unsqueeze(0)\n",
        "        result_baseline = self.baseline_method(train_data, output.shape)\n",
        "        MAE = torch.nn.L1Loss()\n",
        "        mae2 = MAE(output, target)\n",
        "        mase4 = MAE(result_baseline, target)\n",
        "        if mase4 < 0.001:\n",
        "            mase4 = 0.001\n",
        "        return mae2 / mase4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjErN2nj2NyY"
      },
      "source": [
        "## Defining Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dDimPrK3SLZP"
      },
      "outputs": [],
      "source": [
        "pytorch_criterion_dict = {\"MAE\":nn.L1Loss, \"MSE\":nn.MSELoss, \"MASE\":MASELoss}\n",
        "\n",
        "def make_crit(model_params):\n",
        "    \"\"\"\n",
        "    Creates the criterion for training from the parameters\n",
        "    \"\"\"\n",
        "    training_params = model_params\n",
        "    criterion_init_params = {}\n",
        "    if \"criterion_params\" in training_params:\n",
        "        criterion_init_params = training_params[\"criterion_params\"]\n",
        "    if type(training_params[\"criterion\"]) == list:\n",
        "        criterion = []\n",
        "        i = 0\n",
        "        for crit, param in zip(training_params[\"criterion\"], criterion_init_params):\n",
        "            res = pytorch_criterion_dict[crit](**param)\n",
        "            i += 1\n",
        "            criterion.append(res)\n",
        "    else:\n",
        "        criterion = pytorch_criterion_dict[training_params[\"criterion\"]](**criterion_init_params)\n",
        "    return criterion\n",
        "\n",
        "\n",
        "def get_output_std(src, output, labels, probabilistic, output_std):\n",
        "    \"\"\"\n",
        "    Handles un-scaling the model output.\n",
        "    \"\"\"\n",
        "    output_dist = None\n",
        "    if probabilistic:\n",
        "        try:\n",
        "            output_std = Variable(torch.from_numpy(output_std).type(torch.FloatTensor).to(device))\n",
        "        except Exception:\n",
        "            pass\n",
        "        output_dist = torch.distributions.Normal(output, output_std)\n",
        "    return src, output, labels, output_dist\n",
        "\n",
        "\n",
        "def compute_loss(labels, output, src, criterion, validation_dataset, probabilistic=None, output_std=None, m=1):\n",
        "    \"\"\"\n",
        "    Computes loss.\n",
        "    \"\"\"\n",
        "    if not probabilistic and isinstance(output, torch.Tensor):\n",
        "        if len(labels.shape) != len(output.shape):\n",
        "            if len(labels.shape) > 1:\n",
        "                if labels.shape[1] == output.shape[1]:\n",
        "                    labels = labels.unsqueeze(2)\n",
        "                else:\n",
        "                    labels = labels.unsqueeze(0)\n",
        "    if probabilistic:\n",
        "        if type(output_std) != torch.Tensor:\n",
        "            output_std = torch.from_numpy(output_std)\n",
        "        if type(output) != torch.Tensor:\n",
        "            output = torch.from_numpy(output)\n",
        "        output_dist = torch.distributions.Normal(output, output_std)\n",
        "    if validation_dataset:\n",
        "        src, output, labels, output_dist = get_output_std(src, output, labels, probabilistic,output_std)\n",
        "    if probabilistic:\n",
        "        loss = -output_dist.log_prob(labels.float()).sum() \n",
        "    elif isinstance(criterion, MASELoss):\n",
        "        loss = criterion(labels.float(), output, src, m)\n",
        "    else:\n",
        "        loss = criterion(output, labels.float())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def multi_crit(crit_multi: List, output, labels, valid=None):\n",
        "    i = 0\n",
        "    loss = 0.0\n",
        "    for crit in crit_multi:\n",
        "        if len(output.shape) == 3:\n",
        "            loss += compute_loss(labels[:, :, i], output[:, :, i], torch.rand(1, 2), crit, valid)\n",
        "        else:\n",
        "            loss += compute_loss(labels[:, i], output[:, i], torch.rand(1, 2), crit, valid)\n",
        "    summed_loss = loss\n",
        "    return summed_loss\n",
        "\n",
        "\n",
        "def train_step(model, opt, criterion, data_loader, takes_target, device,\n",
        "                       num_targets=1, probablistic=False, forward_params={}):\n",
        "    \"\"\"\n",
        "    Performs training of a single model. Runs through one epoch of the data.\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    output_std = None\n",
        "    running_loss = 0.0\n",
        "    for src, trg, src_orig in data_loader:\n",
        "        opt.zero_grad()\n",
        "        if takes_target:\n",
        "            forward_params[\"t\"] = trg.to(device)\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        output = model(src, **forward_params)\n",
        "        if num_targets == 1:\n",
        "            labels = trg[:, :, 0]\n",
        "        elif num_targets > 1:\n",
        "            labels = trg[:, :, 0:num_targets]\n",
        "        if probablistic:\n",
        "            output1 = output\n",
        "            output = output.mean\n",
        "            output_std = output1.stddev\n",
        "        if type(criterion) == list:\n",
        "            loss = multi_crit(criterion, output, labels, None)\n",
        "        else:\n",
        "            loss = compute_loss(labels, output, src, criterion, None, probablistic, output_std, m=num_targets)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running_loss += loss.item()\n",
        "        i += 1\n",
        "    total_loss = running_loss\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def validation(val_loader, model, criterion, device, decoder_structure=False,\n",
        "                       num_targets=1, probabilistic=False):\n",
        "    \"\"\"\n",
        "    Computes the validation loss metrics\n",
        "    \"\"\"\n",
        "    crit_losses = dict.fromkeys(criterion, 0)\n",
        "    model.eval()\n",
        "    output_std = None\n",
        "    labels = torch.Tensor(0).to(device)\n",
        "    labels_all = torch.Tensor(0).to(device)\n",
        "    output_all = torch.Tensor(0).to(device)\n",
        "    src_all = torch.Tensor(0)\n",
        "    with torch.no_grad():\n",
        "        for src, targ, src_orig in val_loader:\n",
        "            output = torch.Tensor(0).to(device)\n",
        "            src_all = torch.cat((src_all, src_orig[:,:,0]))\n",
        "            src = src if isinstance(src, list) else src.to(device)\n",
        "            targ = targ if isinstance(targ, list) else targ.to(device)\n",
        "            if decoder_structure:\n",
        "                if type(model).__name__ == \"SimpleTransformer\":\n",
        "                    targ_clone = targ.detach().clone()\n",
        "                    output = greedy_decode(model, src, targ.shape[1], targ_clone, device=device)[:,:,0]\n",
        "                    output_all = torch.cat((output_all, output))                                                                                 \n",
        "            else:\n",
        "                if probabilistic:\n",
        "                    output_dist = model(src.float())\n",
        "                    output = output_dist.mean.detach().numpy()\n",
        "                    output_std = output_dist.stddev.detach().numpy()\n",
        "                else:\n",
        "                    output = model(src.float())\n",
        "                    output_all = torch.cat((output_all, output))\n",
        "            if num_targets == 1:\n",
        "                labels = targ[:, :, 0]\n",
        "            elif num_targets > 1:\n",
        "                labels = targ[:, :, 0:num_targets]\n",
        "            for crit in criterion:\n",
        "                loss = compute_loss(labels, output, src, crit, False, probabilistic, output_std, m=num_targets)\n",
        "                crit_losses[crit] += loss.item()\n",
        "            labels_all = torch.cat((labels_all, labels))\n",
        "    return list(crit_losses.values())[0], output_all, labels_all, src_all\n",
        "\n",
        "def forecast(data_loader, model, criterion, forecast_horizon, device, decoder_structure=False,\n",
        "                       num_targets=1, probabilistic=False):\n",
        "    crit_losses = dict.fromkeys(criterion, 0)\n",
        "    model.eval()\n",
        "    output_decoder = torch.Tensor(0).to(device)\n",
        "    labels = torch.Tensor(0).to(device)\n",
        "    labels_all = torch.Tensor(0).to(device)\n",
        "    src_all = torch.Tensor(0)\n",
        "    output_mean, output_std = [], []\n",
        "    counter = 0\n",
        "    with torch.no_grad():\n",
        "        for src, targ, src_orig in data_loader:\n",
        "            if (counter % forecast_horizon) == 0:\n",
        "                src_all = torch.cat((src_all, src_orig[:,:,0]))\n",
        "                src = src if isinstance(src, list) else src.to(device)\n",
        "                targ = targ if isinstance(targ, list) else targ.to(device)\n",
        "                if decoder_structure:\n",
        "                    if type(model).__name__ == \"SimpleTransformer\":\n",
        "                        targ_clone = targ.detach().clone()\n",
        "                        output = greedy_decode(model, src, targ.shape[1], targ_clone, device=device)[:,:,0]\n",
        "                        output_decoder = torch.cat((output_decoder, output))                                                                                 \n",
        "                else:\n",
        "                    if probabilistic:\n",
        "                        output_dist = model(src.float())\n",
        "                        output_mean.append(output_dist.mean.detach().numpy())\n",
        "                        output_std.append(output_dist.stddev.detach().numpy())\n",
        "                    else:\n",
        "                        output = model(src.float())\n",
        "                        output_decoder = torch.cat((output_decoder, output))\n",
        "                if num_targets == 1:\n",
        "                    labels = targ[:, :, 0]\n",
        "                elif num_targets > 1:\n",
        "                    labels = targ[:, :, 0:num_targets]\n",
        "                for crit in criterion:\n",
        "                    loss = compute_loss(labels, output, src, crit, False, probabilistic, output_std, m=num_targets)\n",
        "                    crit_losses[crit] += loss.item()\n",
        "                labels_all = torch.cat((labels_all, labels))\n",
        "            counter += 1\n",
        "    if decoder_structure:\n",
        "        return list(crit_losses.values())[0], output_decoder, labels_all, src_all\n",
        "    elif probabilistic:\n",
        "        return list(crit_losses.values())[0], (output_mean, output_std), labels_all, src_all\n",
        "    else:\n",
        "        return list(crit_losses.values())[0], output_decoder, labels_all, src_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZOqAUud2UNQ"
      },
      "source": [
        "## Defining Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uuQDDNHiFbf4"
      },
      "outputs": [],
      "source": [
        "def strategy_evaluator(true, pred):\n",
        "    '''\n",
        "    Evaluates trading strategy based on correct buys and sells\n",
        "    '''\n",
        "    total_buys, total_sells, total_holds = np.sum(true>0), np.sum(true<0), np.sum(true==0)\n",
        "    total_correct_buys, total_correct_sells, total_correct_holds = 0, 0, 0\n",
        "    for idx in range(len(true)):\n",
        "        for jdx in range(len(true[0])):\n",
        "            if true[idx,jdx] > 0 and pred[idx,jdx] > 0:\n",
        "                total_correct_buys += 1\n",
        "            elif true[idx,jdx] < 0 and pred[idx,jdx] < 0:\n",
        "                total_correct_sells += 1\n",
        "            elif true[idx,jdx] == 0 and pred[idx,jdx] == 0:\n",
        "                total_correct_holds += 1\n",
        "    total_correct_buys_r, total_correct_sells_r, total_correct_holds_r = (total_correct_buys/total_buys),(total_correct_sells/total_sells),(total_correct_holds/total_holds)\n",
        "    return total_correct_buys_r.round(3), total_correct_sells_r.round(3), total_correct_holds_r.round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mUp74e95PcwZ"
      },
      "outputs": [],
      "source": [
        "def trainer(model, train_loader, validation_loader, test_loader, criterion, opt, scheduler,\n",
        "            warmup_scheduler, max_epochs, batch_size, forecast_horizon, takes_target, shuffle=False,\n",
        "            decoder_structure=True, probabilistic=False, num_targets=1, plot_prediction=True, save_path=None, LAG=0):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    data_loader = DataLoader(train_loader, batch_size=batch_size, shuffle=shuffle, sampler=None, batch_sampler=None, num_workers=0)\n",
        "    validation_data_loader = DataLoader(validation_loader, batch_size=batch_size, shuffle=False, sampler=None, batch_sampler=None, num_workers=0)\n",
        "    test_data_loader = DataLoader(test_loader, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0)\n",
        "    forecast_data_loader = DataLoader(validation_loader, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0)\n",
        "    \n",
        "    for epoch in range(1, max_epochs+1):\n",
        "        total_loss = train_step(model, opt, criterion, data_loader, takes_target, device, num_targets=num_targets)\n",
        "        val_loss = 0\n",
        "        if plot_prediction:\n",
        "            val_loss, val_values, true_values, src_all = forecast(forecast_data_loader, model, criterion, forecast_horizon=forecast_horizon,\n",
        "                                                                   device=device, decoder_structure=decoder_structure,\n",
        "                                                                   num_targets=num_targets, probabilistic=probabilistic)\n",
        "            fig, ax = plt.subplots(1, 1, figsize = (18, 8))\n",
        "            ax.plot(true_values.cpu().view(-1), label='truth', alpha=0.3)\n",
        "            ax.plot(val_values.cpu().view(-1), label='forecast', alpha=0.8)\n",
        "            ax.set_xlim(left=0, right=len(true_values.cpu().view(-1)))\n",
        "            plt.show()\n",
        "        else:\n",
        "            val_loss, val_values, true_values, src_all = validation(validation_data_loader, model, criterion, device,\n",
        "                                                             decoder_structure=decoder_structure, num_targets=num_targets,\n",
        "                                                             probabilistic=probabilistic)\n",
        "        \n",
        "        preds, trues, src = val_values.cpu().numpy(), true_values.cpu().numpy(), src_all.cpu().numpy()\n",
        "\n",
        "        print(f'preds {preds.shape}')\n",
        "        print(f'trues {trues.shape}')\n",
        "        print(f'src {src.shape}')\n",
        "\n",
        "        results = strategy_evaluator(trues, preds)\n",
        "        r2_sklearn = r2_score(trues, preds)\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print('-' * 88)\n",
        "        print('| epoch {:3d} | {:5.2f} s | train loss {:5.5f} | val loss {:5.5f} | lr {:1.8f} | r2 sklearn: {:1.5f} | b, s, h: {:}|'.format(\n",
        "                        epoch, elapsed, total_loss, val_loss, scheduler.get_last_lr()[0], r2_sklearn, results))\n",
        "        print('-' * 88)\n",
        "        start_time = time.time()\n",
        "\n",
        "        if save_path:\n",
        "            results = {\n",
        "                    'model': 'Transformer_encdec',\n",
        "                    'pred_len': forecast_horizon,\n",
        "                    'epoch': epoch,\n",
        "                    'train_loss': total_loss,\n",
        "                    'val_loss': val_loss,\n",
        "                    # 'r2_val': r2,\n",
        "                    'r2_val_sklearn': r2_sklearn            \n",
        "            }\n",
        "\n",
        "            df = pd.DataFrame([results])\n",
        "            df.to_csv(save_path, mode='a', header=not os.path.exists(save_path), index=False)\n",
        "\n",
        "        with warmup_scheduler.dampening():\n",
        "            scheduler.step()\n",
        "\n",
        "    decoder_structure = True\n",
        "    # test = validation(test_data_loader, model, [criterion], device, decoder_structure=decoder_structure, num_targets=num_targets,\n",
        "    #                           probabilistic=probabilistic)\n",
        "    # print('| test loss {:5.5f} |'.format(test))\n",
        "    # model.params[\"run\"] = session_params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ5zrDoD2X1k"
      },
      "source": [
        "## Model and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZXgrUySymAj"
      },
      "outputs": [],
      "source": [
        "date_train = 'all' \n",
        "date_test = 'all'\n",
        "\n",
        "if drive:\n",
        "    agg_trade = pd.read_csv(config[\"paths\"][\"drive\"][\"agg_trade\"][\"train\"]+date_train+'/orderbook.csv')\n",
        "    agg_trade = agg_trade[1_000_000:]\n",
        "    \n",
        "    sys.path.append(config[\"paths\"][\"drive\"][\"utils\"])\n",
        "else:\n",
        "    agg_trade = pd.read_csv(config[\"paths\"][\"local\"][\"agg_trade\"][\"train\"]+date_train+'/orderbook_agg_trade_dollarvol.csv')\n",
        "    agg_trade_test = pd.read_csv(config[\"paths\"][\"local\"][\"agg_trade\"][\"test\"]+date_test+'/orderbook_agg_trade_dollarvol.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHnk5zebwY1E"
      },
      "outputs": [],
      "source": [
        "agg_trade['price'] = agg_trade['w_midprice']\n",
        "agg_trade = agg_trade[agg_trade['update_diff']>0]\n",
        "\n",
        "agg_trade_test = agg_trade[2_000_000:]\n",
        "\n",
        "trade = augment_trade_data(agg_trade, lag=30)\n",
        "trade_test = augment_trade_data(agg_trade_test, lag=30)\n",
        "\n",
        "target = 'price'\n",
        "\n",
        "features = [target, 'lag_return',\n",
        "            'bid1', 'bidqty1', 'bid2', 'bidqty2', 'bid3', 'bidqty3', 'bid4', 'bidqty4', 'bid5', 'bidqty5',\n",
        "            'bid6', 'bidqty6', 'bid7', 'bidqty7', 'bid8', 'bidqty8', 'bid9', 'bidqty9',\n",
        "            'ask1', 'askqty1', 'ask2', 'askqty2', 'ask3', 'askqty3', 'ask4', 'askqty4', 'ask5', 'askqty5',\n",
        "            'ask6', 'askqty6', 'ask7', 'askqty7', 'ask8', 'askqty8', 'ask9', 'askqty9']\n",
        "\n",
        "train_data, test_data = select_features(trade, trade_test, features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCks5SkL9Zuq"
      },
      "outputs": [],
      "source": [
        "forecast_history = 100 #100\n",
        "forecast_len = 30\n",
        "epochs = 400\n",
        "batch_size = 128 #64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSOHACsZz7JU"
      },
      "outputs": [],
      "source": [
        "train_loader = CSVDataLoader(df=train_data[features], forecast_history=forecast_history, forecast_length=forecast_len, \n",
        "                             start_stamp=1_000_000, end_stamp=1_090_000, target_col=['log_returns'])\n",
        "val_loader = CSVDataLoader(df=train_data[features], forecast_history=forecast_history, forecast_length=forecast_len,\n",
        "                            start_stamp=1_091_000, end_stamp=1_100_000, target_col=['log_returns'])\n",
        "test_loader = CSVDataLoader(df=test_data[features], forecast_history=forecast_history, forecast_length=forecast_len, start_stamp=2_035_000,\n",
        "                            end_stamp=2_038_000, target_col=['log_returns'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfOfAp918OSt"
      },
      "outputs": [],
      "source": [
        "model_simple = SimpleTransformer(n_time_series=len(features), seq_len=forecast_history, output_seq_len=forecast_len, d_model=64, n_heads=4,\n",
        "                          dropout=0.25, n_encoder_layers=2, n_decoder_layers=1).to(device)\n",
        "\n",
        "criterion = nn.L1Loss(reduction='sum')\n",
        "optimizer = optim.AdamW(model_simple.parameters(), lr=0.01, amsgrad=True) #0.00005\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
        "warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period=1000)\n",
        "\n",
        "# scheduler = ignite.handlers.param_scheduler.create_lr_scheduler_with_warmup(scheduler,\n",
        "#                                                                     warmup_start_value=0.0,\n",
        "#                                                                     warmup_end_value=0.01,\n",
        "#                                                                     warmup_duration=10)\n",
        "\n",
        "trainer(model_simple, train_loader, val_loader, test_loader, [criterion], optimizer, scheduler, warmup_scheduler, epochs, batch_size, forecast_len,\n",
        "        takes_target=True, plot_prediction=True, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq_67bISiIAb"
      },
      "source": [
        "## Optimal paramater search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rq97xGiXkzPs"
      },
      "outputs": [],
      "source": [
        "date_train = 'all' \n",
        "date_test = 'all'\n",
        "\n",
        "if drive:\n",
        "    agg_trade = pd.read_csv(config[\"paths\"][\"drive\"][\"agg_trade\"][\"train\"]+date_train+'/orderbook.csv')    \n",
        "    sys.path.append(config[\"paths\"][\"drive\"][\"utils\"])\n",
        "else:\n",
        "    agg_trade = pd.read_csv(config[\"paths\"][\"local\"][\"agg_trade\"][\"train\"]+date_train+'/orderbook_agg_trade_dollarvol.csv')\n",
        "    agg_trade_test = pd.read_csv(config[\"paths\"][\"local\"][\"agg_trade\"][\"test\"]+date_test+'/orderbook_agg_trade_dollarvol.csv')\n",
        "\n",
        "agg_trade['price'] = agg_trade['w_midprice']\n",
        "agg_trade_test = agg_trade[4_500_000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1oD9yQb9g-Ov",
        "outputId": "b0f4cdc6-5e0c-4d2a-e8a0-9aa5769776f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "preds (19898, 1)\n",
            "trues (19898, 1)\n",
            "src (19898, 100)\n",
            "----------------------------------------------------------------------------------------\n",
            "| epoch   1 | 54.52 s | train loss 33746.22851 | val loss 13625.75426 | lr 0.01000000 | r2 sklearn: -0.23243 | b, s, h: (0.497, 0.626, 0.0)|\n",
            "----------------------------------------------------------------------------------------\n",
            "preds (19898, 1)\n",
            "trues (19898, 1)\n",
            "src (19898, 100)\n",
            "----------------------------------------------------------------------------------------\n",
            "| epoch   2 | 54.21 s | train loss 12644.75581 | val loss 37027.56632 | lr 0.00980000 | r2 sklearn: -2.34909 | b, s, h: (0.48, 0.593, 0.0)|\n",
            "----------------------------------------------------------------------------------------\n",
            "preds (19898, 1)\n",
            "trues (19898, 1)\n",
            "src (19898, 100)\n",
            "----------------------------------------------------------------------------------------\n",
            "| epoch   3 | 53.52 s | train loss 6047.16074 | val loss 50438.93382 | lr 0.00960400 | r2 sklearn: -3.56213 | b, s, h: (0.476, 0.59, 0.0)|\n",
            "----------------------------------------------------------------------------------------\n",
            "preds (19898, 1)\n",
            "trues (19898, 1)\n",
            "src (19898, 100)\n",
            "----------------------------------------------------------------------------------------\n",
            "| epoch   4 | 53.58 s | train loss 4699.50806 | val loss 53013.96235 | lr 0.00941192 | r2 sklearn: -3.79504 | b, s, h: (0.475, 0.588, 0.0)|\n",
            "----------------------------------------------------------------------------------------\n",
            "preds (19898, 1)\n",
            "trues (19898, 1)\n",
            "src (19898, 100)\n",
            "----------------------------------------------------------------------------------------\n",
            "| epoch   5 | 53.36 s | train loss 3897.34527 | val loss 53281.92531 | lr 0.00922368 | r2 sklearn: -3.81927 | b, s, h: (0.476, 0.586, 0.0)|\n",
            "----------------------------------------------------------------------------------------\n",
            "preds (19898, 1)\n",
            "trues (19898, 1)\n",
            "src (19898, 100)\n",
            "----------------------------------------------------------------------------------------\n",
            "| epoch   6 | 53.14 s | train loss 3174.59714 | val loss 52610.14047 | lr 0.00903921 | r2 sklearn: -3.75851 | b, s, h: (0.478, 0.586, 0.0)|\n",
            "----------------------------------------------------------------------------------------\n",
            "preds (19898, 1)\n",
            "trues (19898, 1)\n",
            "src (19898, 100)\n",
            "----------------------------------------------------------------------------------------\n",
            "| epoch   7 | 53.43 s | train loss 2360.09666 | val loss 51980.63889 | lr 0.00885842 | r2 sklearn: -3.70157 | b, s, h: (0.48, 0.584, 0.0)|\n",
            "----------------------------------------------------------------------------------------\n",
            "preds (19898, 1)\n",
            "trues (19898, 1)\n",
            "src (19898, 100)\n",
            "----------------------------------------------------------------------------------------\n",
            "| epoch   8 | 53.23 s | train loss 1705.25327 | val loss 50545.31161 | lr 0.00868126 | r2 sklearn: -3.57175 | b, s, h: (0.478, 0.587, 0.0)|\n",
            "----------------------------------------------------------------------------------------\n",
            "preds (19898, 1)\n",
            "trues (19898, 1)\n",
            "src (19898, 100)\n",
            "----------------------------------------------------------------------------------------\n",
            "| epoch   9 | 54.14 s | train loss 1352.36313 | val loss 48923.33583 | lr 0.00850763 | r2 sklearn: -3.42505 | b, s, h: (0.478, 0.588, 0.0)|\n",
            "----------------------------------------------------------------------------------------\n",
            "preds (19898, 1)\n",
            "trues (19898, 1)\n",
            "src (19898, 100)\n",
            "----------------------------------------------------------------------------------------\n",
            "| epoch  10 | 53.64 s | train loss 1139.46891 | val loss 47771.27343 | lr 0.00833748 | r2 sklearn: -3.32084 | b, s, h: (0.477, 0.588, 0.0)|\n",
            "----------------------------------------------------------------------------------------\n",
            "preds (19898, 1)\n",
            "trues (19898, 1)\n",
            "src (19898, 100)\n",
            "----------------------------------------------------------------------------------------\n",
            "| epoch  11 | 54.08 s | train loss 993.68476 | val loss 47150.29320 | lr 0.00817073 | r2 sklearn: -3.26468 | b, s, h: (0.477, 0.587, 0.0)|\n",
            "----------------------------------------------------------------------------------------\n",
            "preds (19898, 1)\n",
            "trues (19898, 1)\n",
            "src (19898, 100)\n",
            "----------------------------------------------------------------------------------------\n",
            "| epoch  12 | 53.24 s | train loss 887.21200 | val loss 47048.37462 | lr 0.00800731 | r2 sklearn: -3.25546 | b, s, h: (0.477, 0.587, 0.0)|\n",
            "----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-da68e06039a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     trainer(model_simple, train_loader, val_loader, test_loader, [criterion], optimizer, scheduler, warmup_scheduler, epochs, batch_size, forecast_horizon=1,\n\u001b[0;32m---> 44\u001b[0;31m         takes_target=True, plot_prediction=False, shuffle=False, save_path=save_path, LAG=forecast_window)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# trainer(model_custom, train_loader, val_loader, test_loader, [criterion], optimizer, scheduler, warmup_scheduler, epochs, batch_size,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-f142abf173ce>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(model, train_loader, validation_loader, test_loader, criterion, opt, scheduler, warmup_scheduler, max_epochs, batch_size, forecast_horizon, takes_target, shuffle, decoder_structure, probabilistic, num_targets, plot_prediction, save_path, LAG)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakes_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_targets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mplot_prediction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-a0c0ad3c43c5>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, opt, criterion, data_loader, takes_target, device, num_targets, probablistic, forward_params)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0moutput_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_orig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtakes_target\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-b7107db0919a>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mtarg_rows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture_return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0msrc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0msrc_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0msrc_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0msrc_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_std\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_median\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mstd\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m   3580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3581\u001b[0m     return _methods._std(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[0;32m-> 3582\u001b[0;31m                          **kwargs)\n\u001b[0m\u001b[1;32m   3583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_std\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    261\u001b[0m          where=True):\n\u001b[1;32m    262\u001b[0m     ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[0;32m--> 263\u001b[0;31m                keepdims=keepdims, where=where)\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         ret = um.true_divide(\n\u001b[0;32m--> 252\u001b[0;31m                 ret, rcount, out=ret, casting='unsafe', subok=False)\n\u001b[0m\u001b[1;32m    253\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "save_path = os.path.join('/content/drive/MyDrive/IP/Repos/HFTransformer/results_normal',\n",
        "                            str(int(time.time()))+'ablation_encoder_results.csv')\n",
        "# save_path=None\n",
        "\n",
        "forecast_history = 100 \n",
        "epochs = 30\n",
        "batch_size = 64 #64 for linear decoder\n",
        "\n",
        "forecast_windows = [i for i in range(2,31)]\n",
        "\n",
        "for forecast_window in forecast_windows:\n",
        "\n",
        "    trade = augment_trade_data(agg_trade, lag=0, forecast_window=forecast_window)\n",
        "    trade_test = augment_trade_data(agg_trade_test, lag=0, forecast_window=forecast_window)\n",
        "    target = 'price' #log_lag'+str(forecast_window)+'_price'\n",
        "\n",
        "    features = [target, 'lag_return',\n",
        "            'bid1', 'bidqty1', 'bid2', 'bidqty2', 'bid3', 'bidqty3', 'bid4', 'bidqty4', 'bid5', 'bidqty5',\n",
        "            'bid6', 'bidqty6', 'bid7', 'bidqty7', 'bid8', 'bidqty8', 'bid9', 'bidqty9',\n",
        "            'ask1', 'askqty1', 'ask2', 'askqty2', 'ask3', 'askqty3', 'ask4', 'askqty4', 'ask5', 'askqty5',\n",
        "            'ask6', 'askqty6', 'ask7', 'askqty7', 'ask8', 'askqty8', 'ask9', 'askqty9']\n",
        "\n",
        "    trade, trade_test = select_features(trade, trade_test, features)\n",
        "\n",
        "    train_loader = CSVDataLoader(df=trade[features], forecast_history=forecast_history, forecast_length=1, \n",
        "                             start_stamp=1_000_000, end_stamp=1_080_000, target_col=['log_returns'], LAG=forecast_window)\n",
        "    val_loader = CSVDataLoader(df=trade[features], forecast_history=forecast_history, forecast_length=1,\n",
        "                                start_stamp=1_080_001, end_stamp=1_100_000, target_col=['log_returns'], LAG=forecast_window)\n",
        "    test_loader = CSVDataLoader(df=trade_test[features], forecast_history=forecast_history, forecast_length=1,\n",
        "                                start_stamp=2_035_000, end_stamp=2_031_000, target_col=['log_returns'], LAG=forecast_window)\n",
        "\n",
        "    model_simple = SimpleTransformer(n_time_series=len(features), seq_len=forecast_history, output_seq_len=1, d_model=80, n_heads=8,\n",
        "                            dropout=0.3, n_encoder_layers=1, n_decoder_layers=1, dim_feedforward=128).to(device)\n",
        "\n",
        "    criterion = nn.MSELoss(reduction='sum')\n",
        "    optimizer = optim.AdamW(model_simple.parameters(), lr=0.0001, amsgrad=True)\n",
        "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
        "    warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period=1000)\n",
        "\n",
        "    trainer(model_simple, train_loader, val_loader, test_loader, [criterion], optimizer, scheduler, warmup_scheduler, epochs, batch_size, forecast_horizon=1,\n",
        "        takes_target=True, plot_prediction=False, shuffle=False, save_path=save_path, LAG=forecast_window)\n",
        "\n",
        "    print(f'Done with prediction len {forecast_window}.')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "4_1_predicting_ts_transformer.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
